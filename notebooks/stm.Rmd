---
title: "Navigo Topic Model"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load dataset

```{r}
reddit_waze_path <- file.path(getwd(), "../raw/raw_textonly_waze_2019.json")
reddit_waze_raw <- read_json(reddit_waze_path, simplifyVector = TRUE)
```

## Tidy Text

```{r}
reddit_waze_tidy <- reddit_waze_raw %>% 
  replace_with_na_all(condition = ~.x %in% common_na_strings) %>% 
  replace_with_na_all(condition = ~.x == "[deleted]") %>% 
  drop_na(body) %>% 
  mutate(body = str_replace_all(body, 
                                pattern=regex("(www|https?[^\\s]+)"), 
                                replacement = ""), #rm urls
         body = str_replace_all(body, "&#x27;|&quot;|&#x2F;", "'"), ## weird encoding
         body = str_replace_all(body, "<a(.*?)>", " "),             ## links 
         body = str_replace_all(body, "&gt;|&lt;|&amp;", " "),      ## html yuck
         body = str_replace_all(body, "&#[:digit:]+;", " "),        ## html yuck
         body = str_remove_all(body, "<[^>]*>")) 

reddit_waze_tidy <- reddit_waze_tidy %>% 
  unnest_tokens(word, body, token = "words") %>% 
  anti_join(get_stopwords()) %>% 
  filter(!str_detect(word, "[0-9]+")) %>% 
  add_count(word) %>% 
  filter(n > 100) %>% 
  select(-n) %>% 
  count(id, word) %>% 
  cast_sparse(id, word, n)
```

## Train STM

```{r}
many_models <- data_frame(K = c(25, 50, 100, 200)) %>%
  mutate(topic_model = future_map(K, ~stm(reddit_waze_tidy, K = ., verbose = FALSE)))
```

## Explore good K

```{r}
heldout <- make.heldout(reddit_waze_tidy)

k_result <- many_models %>%
  mutate(exclusivity = map(topic_model, exclusivity),
         semantic_coherence = map(topic_model, semanticCoherence, reddit_waze_tidy),
         eval_heldout = map(topic_model, eval.heldout, heldout$missing),
         residual = map(topic_model, checkResiduals, reddit_waze_tidy),
         bound =  map_dbl(topic_model, function(x) max(x$convergence$bound)),
         lfact = map_dbl(topic_model, function(x) lfactorial(x$settings$dim$K)),
         lbound = bound + lfact,
         iterations = map_dbl(topic_model, function(x) length(x$convergence$bound)))

k_result
```

## Plot Model diagnostics

```{r}
k_result %>%
  transmute(K,
            `Lower bound` = lbound,
            Residuals = map_dbl(residual, "dispersion"),
            `Semantic coherence` = map_dbl(semantic_coherence, mean),
            `Held-out likelihood` = map_dbl(eval_heldout, "expected.heldout")) %>%
  gather(Metric, Value, -K) %>%
  ggplot(aes(K, Value, color = Metric)) +
  geom_line(size = 1.5, alpha = 0.7, show.legend = FALSE) +
  facet_wrap(~Metric, scales = "free_y") +
  labs(x = "K (number of topics)",
       y = NULL,
       title = "Model diagnostics by number of topics",
       subtitle = "These diagnostics indicate that a good number of topics would be around 48") +
  theme_tufte(base_family = "Arial", ticks = FALSE)
```

## Compare Exclusivity and Semantic Coherence

```{r}
k_result %>%
  select(K, exclusivity, semantic_coherence) %>%
  filter(K %in% c(50, 100, 200)) %>%
  unnest() %>%
  mutate(K = as.factor(K)) %>%
  ggplot(aes(semantic_coherence, exclusivity, color = K)) +
  geom_point(size = 2, alpha = 0.7) +
  labs(x = "Semantic coherence",
       y = "Exclusivity",
       title = "Comparing exclusivity and semantic coherence",
       subtitle = "Models with fewer topics have higher semantic coherence for more topics, but lower exclusivity")
```

## Choose a the topic model with best K

```{r}
topic_model <- k_result %>% 
  filter(K == 50) %>% 
  pull(topic_model) %>% 
  .[[1]]

topic_model
```


## Explore the Topic Model

```{r}
td_beta <- tidy(topic_model)

td_beta %>%
    group_by(topic) %>%
    # filter(topic %in% c(19, 37, 78)) %>% 
    top_n(10, beta) %>%
    ungroup() %>%
    mutate(topic = paste0("Topic ", topic),
           term = reorder_within(term, beta, topic)) %>%
    ggplot(aes(term, beta, fill = as.factor(topic))) +
    geom_col(alpha = 0.8, show.legend = FALSE) +
    facet_wrap(~ topic, scales = "free_y") +
    coord_flip() +
    scale_x_reordered() +
    labs(x = NULL, y = expression(beta),
         title = "Highest word probabilities for each topic",
         subtitle = "Different words are associated with different topics") +
    theme_tufte()
```

## Probability that each document is generated from a topic

```{r}
td_gamma <- tidy(topic_model, matrix = "gamma",
                 document_names = rownames(reddit_waze_tidy))

td_gamma
```


## Topic prevalence and top words per topic

```{r}
library(ggthemes)

top_terms <- td_beta %>%
  arrange(beta) %>%
  group_by(topic) %>%
  top_n(7, beta) %>%
  arrange(-beta) %>%
  select(topic, term) %>%
  summarise(terms = list(term)) %>%
  mutate(terms = map(terms, paste, collapse = ", ")) %>% 
  unnest()

gamma_terms <- td_gamma %>%
  group_by(topic) %>%
  summarise(gamma = mean(gamma)) %>%
  arrange(desc(gamma)) %>%
  left_join(top_terms, by = "topic") %>%
  mutate(topic = paste0("Topic ", topic),
         topic = reorder(topic, gamma))

gamma_terms %>%
  top_n(20, gamma) %>%
  ggplot(aes(topic, gamma, label = terms, fill = topic)) +
  geom_col(show.legend = FALSE) +
  geom_text(hjust = 0, nudge_y = 0.001, size = 2,
            family = "Avenir") +
  coord_flip() +
  scale_y_continuous(expand = c(0,0),
                     limits = c(0, 0.05),
                     labels = percent_format()) +
  theme_tufte(base_family = "Avenir", ticks = FALSE) +
  theme(plot.title = element_text(size = 16,
                                  family="Avenir"),
        plot.subtitle = element_text(size = 13), axis.text.y = element_text(size = 5)) +
  labs(x = NULL, y = expression(gamma),
       title = "Topic prevalence across article titles",
       subtitle = "With the top 7 words that contribute to each topic")
```

## Gamma ordered by prevalence

```{r}
gamma_terms %>%
  select(topic, gamma, terms) %>%
  kable(digits = 3, 
        col.names = c("Topic", "Expected topic proportion", "Top 7 terms"))
```
